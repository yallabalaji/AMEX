{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2207f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f435e813-d178-4efa-bdfc-dab7a068e1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bf34f13-0ac8-452a-b9e6-f7693bd0126f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f9fcf0-159e-42d5-a1fc-0449a43d9c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./data\")\n",
    "train_data_path = DATA_DIR / \"train_data.csv\"\n",
    "train_labels_path = DATA_DIR / 'train_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd9f1d6d-26e4-49f6-9dd9-dc5d1ad828dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 100_000  # Tune based on available RAM\n",
    "chunks = pd.read_csv(train_data_path, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceeacd6-7542-41d8-bbac-b9bfaedd3543",
   "metadata": {},
   "source": [
    "This below function will give object level memory utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8fa8a80-0966-4536-8d40-6fc9beedd726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import humanize\n",
    "\n",
    "def print_variable_sizes(top_n=None, scope=None):\n",
    "    \"\"\"\n",
    "    Prints all variables in memory (default: globals()), sorted by size (largest first).\n",
    "    \n",
    "    Args:\n",
    "        top_n (int): Optional. Show only top N variables.\n",
    "        scope (dict): Optional. Dictionary to inspect (e.g., locals() or globals()).\n",
    "    \"\"\"\n",
    "    if scope is None:\n",
    "        scope = globals()\n",
    "\n",
    "    var_list = []\n",
    "    for k, v in scope.items():\n",
    "        if k.startswith('_'):\n",
    "            continue  # skip internal vars\n",
    "        try:\n",
    "            size = sys.getsizeof(v)\n",
    "            var_list.append((k, type(v).__name__, size, humanize.naturalsize(size)))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    var_list.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(f\"{'Variable':<20} {'Type':<20} {'Size':<15}\")\n",
    "    print(\"-\" * 55)\n",
    "    for i, (name, vtype, size, size_str) in enumerate(var_list):\n",
    "        if top_n is not None and i >= top_n:\n",
    "            break\n",
    "        print(f\"{name:<20} {vtype:<20} {size_str:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62c2f00e-0b36-4cf8-91ae-bf022fc862d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "float16_columns = ['D_66', 'D_68', 'B_30', 'D_87', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126']\n",
    "float32_columns = ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42', 'D_43', 'D_44', 'B_4', 'D_45', 'B_5', 'R_2', 'D_46', 'D_47', 'D_48', 'D_49', 'B_6', 'B_7', 'B_8', 'D_50', 'D_51', 'B_9', 'R_3', 'D_52', 'P_3', 'B_10', 'D_53', 'S_5', 'B_11', 'S_6', 'D_54', 'R_4', 'S_7', 'B_12', 'S_8', 'D_55', 'D_56', 'B_13', 'R_5', 'D_58', 'S_9', 'B_14', 'D_59', 'D_60', 'D_61', 'B_15', 'S_11', 'D_62', 'D_65', 'B_16', 'B_17', 'B_18', 'B_19', 'B_20', 'S_12', 'R_6', 'S_13', 'B_21', 'D_69', 'B_22', 'D_70', 'D_71', 'D_72', 'S_15', 'B_23', 'D_73', 'P_4', 'D_74', 'D_75', 'D_76', 'B_24', 'R_7', 'D_77', 'B_25', 'B_26', 'D_78', 'D_79', 'R_8', 'R_9', 'S_16', 'D_80', 'R_10', 'R_11', 'B_27', 'D_81', 'D_82', 'S_17', 'R_12', 'B_28', 'R_13', 'D_83', 'R_14', 'R_15', 'D_84', 'R_16', 'B_29', 'S_18', 'D_86', 'R_17', 'R_18', 'D_88', 'S_19', 'R_19', 'B_32', 'S_20', 'R_20', 'R_21', 'B_33', 'D_89', 'R_22', 'R_23', 'D_91', 'D_92', 'D_93', 'D_94', 'R_24', 'R_25', 'D_96', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'B_36', 'B_37', 'R_26', 'R_27', 'D_108', 'D_109', 'D_110', 'D_111', 'B_39', 'D_112', 'B_40', 'S_27', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_127', 'D_128', 'D_129', 'B_41', 'B_42', 'D_130', 'D_131', 'D_132', 'D_133', 'R_28', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145']\n",
    "bool_cols = ['B_31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b7896e-f20a-4e90-8e91-1b9877da876f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "high_corr_cols = ['D_111', 'D_110', 'B_39', 'D_134', 'D_135', 'D_136', 'D_137',\n",
    "                         'D_138', 'R_9', 'D_106', 'D_132', 'D_49', 'R_26', 'D_76',\n",
    "                         'D_66', 'D_42', 'D_142', 'D_53', 'D_82']\n",
    "\n",
    "low_missing_corr_cols = ['D_87', 'D_88', 'D_108', 'D_73', 'B_42', 'B_29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78216924-87fc-40d3-946a-283e10bae6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cardinality_json = '{\"D_87\":1,\"D_120\":2,\"D_66\":2,\"D_116\":2,\"D_114\":2,\"D_126\":3,\"B_30\":3,\"D_117\":7,\"B_38\":7}'\n",
    "cardinality_dict = json.loads(cardinality_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7fbc8f-0f49-461f-9bfc-b946612a92cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_cols = list(cardinality_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50bd165b-4a44-4668-84e8-059bd7dc48b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_high_corr_missingness(df: pd.DataFrame, high_corr_cols: list) -> pd.DataFrame:\n",
    "    # 1. Create missingness flags\n",
    "    missing_flag_cols = {\n",
    "        f\"{col}_was_missing\": df[col].isna().astype(np.uint8)\n",
    "        for col in high_corr_cols if col in df.columns\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame(missing_flag_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # 2. Fill NaNs with sentinel values based on dtype\n",
    "    for col in high_corr_cols:\n",
    "        if col in df.columns:\n",
    "            col_dtype = df[col].dtype\n",
    "\n",
    "            if isinstance(col_dtype, pd.CategoricalDtype):\n",
    "                base_type = df[col].cat.categories.dtype\n",
    "\n",
    "                if pd.api.types.is_numeric_dtype(base_type):\n",
    "                    sentinel = -999\n",
    "                    # Add only if sentinel not already in categories\n",
    "                    if sentinel not in df[col].cat.categories:\n",
    "                        df[col] = df[col].cat.add_categories([sentinel])\n",
    "                    df[col] = df[col].fillna(sentinel)\n",
    "\n",
    "                else:  # assume string-based\n",
    "                    sentinel = \"MISSING\"\n",
    "                    if sentinel not in df[col].cat.categories:\n",
    "                        df[col] = df[col].cat.add_categories([sentinel])\n",
    "                    df[col] = df[col].fillna(sentinel)\n",
    "\n",
    "            elif pd.api.types.is_float_dtype(col_dtype):\n",
    "                sentinel = -999.0\n",
    "                df[col] = df[col].fillna(sentinel)\n",
    "\n",
    "            elif pd.api.types.is_integer_dtype(col_dtype):\n",
    "                sentinel = -1\n",
    "                df[col] = df[col].fillna(sentinel)\n",
    "\n",
    "            else:\n",
    "                # Fallback for any unexpected dtype\n",
    "                df[col] = df[col].fillna(\"MISSING\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eaed3c9-409f-4bc1-b6c2-7a8970e15c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_chunk(chunk: pd.DataFrame, model_type: str = \"tree\") -> pd.DataFrame:\n",
    "    # Boolean conversions\n",
    "    for col in bool_cols:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].astype(bool)\n",
    "\n",
    "    # Float16 conversions and median imputation\n",
    "    for col in float16_columns:\n",
    "        if col in chunk.columns:\n",
    "            median_val = chunk[col].median()\n",
    "            chunk[col] = chunk[col].fillna(median_val).astype('float16')\n",
    "\n",
    "    # Float32 conversions and median imputation\n",
    "    for col in float32_columns:\n",
    "        if col in chunk.columns:\n",
    "            median_val = chunk[col].median()\n",
    "            chunk[col] = chunk[col].fillna(median_val).astype('float32')\n",
    "\n",
    "    # String conversion\n",
    "    if 'customer_ID' in chunk.columns:\n",
    "        chunk['customer_ID'] = chunk['customer_ID'].astype('string')\n",
    "\n",
    "    # Datetime conversion\n",
    "    if 'S_2' in chunk.columns:\n",
    "        chunk['S_2'] = pd.to_datetime(chunk['S_2'])\n",
    "\n",
    "    # Fixed known categorical columns and imputation\n",
    "    for col in ['D_63', 'D_64']:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].astype('category')\n",
    "            if col == 'D_64':\n",
    "                if 'MISSING' not in chunk[col].cat.categories:\n",
    "                    chunk[col] = chunk[col].cat.add_categories('MISSING')\n",
    "                chunk[col] = chunk[col].fillna('MISSING')\n",
    "            else:\n",
    "                mode_val = chunk[col].mode()\n",
    "                if not mode_val.empty:\n",
    "                    chunk[col] = chunk[col].fillna(mode_val[0])\n",
    "\n",
    "    # Other categorical columns and mode imputation\n",
    "    for col in categorical_cols:\n",
    "        if col in chunk.columns:\n",
    "            # If float16, convert first to float32 before category\n",
    "            if pd.api.types.is_float_dtype(chunk[col].dtype) and chunk[col].dtype == 'float16':\n",
    "                chunk[col] = chunk[col].astype('float32')\n",
    "            chunk[col] = chunk[col].astype('category')\n",
    "            mode_val = chunk[col].mode()\n",
    "            if not mode_val.empty:\n",
    "                chunk[col] = chunk[col].fillna(mode_val[0])\n",
    "\n",
    "    # Drop low missing correlation columns\n",
    "    chunk = chunk.drop(columns=[col for col in low_missing_corr_cols if col in chunk.columns])\n",
    "\n",
    "    # Handle high correlation missingness\n",
    "    chunk = handle_high_corr_missingness(chunk, high_corr_cols)\n",
    "\n",
    "    # Model-specific processing\n",
    "    if model_type == \"linear\":\n",
    "        cat_cols = [col for col in categorical_cols if col in chunk.columns]\n",
    "        if cat_cols:\n",
    "            chunk = pd.get_dummies(chunk, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    return chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e20573b-eeeb-454e-8dd6-4dca48981b48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_and_save_parquet(input_csv: str, output_prefix: str, chunksize: int = 100_000):\n",
    "    refined_dir = os.path.join(os.path.dirname(output_prefix), \"refined_data\")\n",
    "    os.makedirs(refined_dir, exist_ok=True)\n",
    "    chunks = pd.read_csv(input_csv, chunksize=chunksize)\n",
    "    parquet_parts = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        processed = preprocess_chunk(chunk, model_type=\"tree\")  # always save raw categories\n",
    "        filename = f\"{os.path.basename(output_prefix)}_part{i}.parquet\"\n",
    "        part_path = os.path.join(refined_dir, filename)\n",
    "        processed.to_parquet(part_path, index=False)\n",
    "        parquet_parts.append(part_path)\n",
    "\n",
    "    return parquet_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2563906-b319-47e0-8743-244c9d385a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_for_linear(parquet_paths: list):\n",
    "    dfs = []\n",
    "    for path in parquet_paths:\n",
    "        df = pd.read_parquet(path)\n",
    "        cat_cols = [col for col in categorical_cols if col in df.columns]\n",
    "        if cat_cols:\n",
    "            df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099954f4-249b-4c34-8178-eca9f899474d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_for_tree(parquet_paths: list):\n",
    "    dfs = [pd.read_parquet(path) for path in parquet_paths]\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a8a757d-f920-44af-a075-afc0e84f69b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: preprocess large CSV and save Parquet parts\n",
    "parquet_files = preprocess_and_save_parquet(train_data_path, \"processed_data\")\n",
    "\n",
    "# Step 2a: load for linear model (one-hot encoding done here)\n",
    "linear_df = load_and_prepare_for_linear(parquet_files)\n",
    "\n",
    "# Step 2b: load for tree model (categorical dtypes preserved)\n",
    "tree_df = load_and_prepare_for_tree(parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d629fe8-da5e-4577-aaec-ad438e920155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable             Type                 Size           \n",
      "-------------------------------------------------------\n",
      "tree_df              DataFrame            4.8 GB         \n",
      "linear_df            DataFrame            4.7 GB         \n",
      "float32_columns      list                 1.7 kB         \n",
      "Path                 type                 952 Bytes      \n",
      "parquet_files        list                 568 Bytes      \n",
      "cardinality_dict     dict                 272 Bytes      \n",
      "In                   list                 248 Bytes      \n",
      "high_corr_cols       list                 216 Bytes      \n",
      "open                 function             168 Bytes      \n",
      "print_variable_sizes function             168 Bytes      \n",
      "handle_high_corr_missingness function             168 Bytes      \n",
      "preprocess_chunk     function             168 Bytes      \n",
      "preprocess_and_save_parquet function             168 Bytes      \n",
      "load_and_prepare_for_linear function             168 Bytes      \n",
      "load_and_prepare_for_tree function             168 Bytes      \n",
      "categorical_cols     list                 152 Bytes      \n",
      "float16_columns      list                 136 Bytes      \n",
      "cardinality_json     str                  128 Bytes      \n",
      "DATA_DIR             PosixPath            104 Bytes      \n",
      "train_data_path      PosixPath            104 Bytes      \n",
      "train_labels_path    PosixPath            104 Bytes      \n",
      "low_missing_corr_cols list                 104 Bytes      \n",
      "np                   module               72 Bytes       \n",
      "pd                   module               72 Bytes       \n",
      "json                 module               72 Bytes       \n",
      "sys                  module               72 Bytes       \n",
      "humanize             module               72 Bytes       \n",
      "Out                  dict                 64 Bytes       \n",
      "get_ipython          method               64 Bytes       \n",
      "bool_cols            list                 64 Bytes       \n",
      "exit                 ZMQExitAutocall      48 Bytes       \n",
      "quit                 ZMQExitAutocall      48 Bytes       \n",
      "chunks               TextFileReader       48 Bytes       \n",
      "chunk_size           int                  28 Bytes       \n"
     ]
    }
   ],
   "source": [
    "print_variable_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc1e59a7-3584-450e-818e-d65c808301a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree_df rows: 5,531,451\n",
      "linear_df rows: 5,531,451\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree_df rows: {len(tree_df):,}\")\n",
    "print(f\"linear_df rows: {len(linear_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85de0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./\")\n",
    "os.makedirs(DATA_DIR/\"stage1\", exist_ok=True)\n",
    "STAGE1_DIR = DATA_DIR/\"stage1\"\n",
    "tree_df.to_parquet(STAGE1_DIR/\"tree_df.parquet\",compression=\"snappy\", index=False)\n",
    "linear_df.to_parquet(STAGE1_DIR/\"linear_df.parquet\", compression=\"snappy\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
